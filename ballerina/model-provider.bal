// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/ai;
import ballerina/jballerina.java;
import ballerinax/azure.openai.chat;

const DEFAULT_MAX_TOKEN_COUNT = 512;
const DEFAULT_TEMPERATURE = 0.7d;

# OpenAiProvider is a client class that provides an interface for interacting with Azure-hosted OpenAI language models.
public isolated client class OpenAiModelProvider {
    *ai:ModelProvider;
    private final chat:Client llmClient;
    private final string deploymentId;
    private final string apiVersion;
    private final decimal temperature;
    private final int maxTokens;

    # Initializes the Azure OpenAI model with the given connection configuration and model configuration.
    #
    # + serviceUrl - The base URL of Azure OpenAI API endpoint
    # + apiKey - The Azure OpenAI API key
    # + deploymentId - The deployment identifier for the specific model deployment in Azure  
    # + apiVersion - The Azure OpenAI API version (e.g., "2023-07-01-preview")
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `ai:Error`
    public isolated function init(@display {label: "Service URL"} string serviceUrl,
            @display {label: "API Key"} string apiKey,
            @display {label: "Deployment ID"} string deploymentId,
            @display {label: "API Version"} string apiVersion,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns ai:Error? {

        chat:ClientHttp1Settings?|error http1Settings = connectionConfig?.http1Settings.cloneWithType();
        if http1Settings is error {
            return error ai:Error("Failed to clone http1Settings", http1Settings);
        }
        // Merge your local connection config with the required auth config
        chat:ConnectionConfig azureAiConfig = {
            auth: {apiKey},
            httpVersion: connectionConfig.httpVersion,
            http1Settings: http1Settings,
            http2Settings: connectionConfig.http2Settings,
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig.poolConfig,
            cache: connectionConfig.cache,
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig.circuitBreaker,
            retryConfig: connectionConfig.retryConfig,
            responseLimits: connectionConfig.responseLimits,
            secureSocket: connectionConfig.secureSocket,
            proxy: connectionConfig.proxy,
            validation: connectionConfig.validation
        };
        chat:Client|error llmClient = new (azureAiConfig, serviceUrl);
        if llmClient is error {
            return error ai:Error("Failed to initialize AzureOpenAiProvider", llmClient);
        }
        self.llmClient = llmClient;
        self.deploymentId = deploymentId;
        self.apiVersion = apiVersion;
        self.temperature = temperature;
        self.maxTokens = maxTokens;
    }

    # Sends a chat request to the OpenAI model with the given messages and tools.
    #
    # + messages - List of chat messages or a user message
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ai:ChatMessage[]|ai:ChatUserMessage messages, ai:ChatCompletionFunctions[] tools, string? stop = ())
        returns ai:ChatAssistantMessage|ai:Error {
        chat:CreateChatCompletionRequest request = {
            stop,
            messages: check self.mapToChatCompletionRequestMessage(messages),
            temperature: self.temperature,
            max_tokens: self.maxTokens
        };
        if tools.length() > 0 {
            request.functions = tools;
        }
        chat:CreateChatCompletionResponse|error response =
            self.llmClient->/deployments/[self.deploymentId]/chat/completions.post(self.apiVersion, request);
        if response is error {
            return error ai:LlmConnectionError("Error while connecting to the model", response);
        }

        record {|
            chat:ChatCompletionResponseMessage message?;
            chat:ContentFilterChoiceResults content_filter_results?;
            int index?;
            string finish_reason?;
            anydata...;
        |}[]? choices = response.choices;

        if choices is () || choices.length() == 0 {
            return error ai:LlmInvalidResponseError("Empty response from the model when using function call API");
        }
        chat:ChatCompletionResponseMessage? message = choices[0].message;
        ai:ChatAssistantMessage chatAssistantMessage = {role: ai:ASSISTANT, content: message?.content};
        chat:ChatCompletionFunctionCall? functionCall = message?.function_call;
        if functionCall is chat:ChatCompletionFunctionCall {
            chatAssistantMessage.toolCalls = [check self.mapToFunctionCall(functionCall)];
        }
        return chatAssistantMessage;
    }

    # Sends a chat request to the model and generates a value that belongs to the type
    # corresponding to the type descriptor argument.
    # 
    # + prompt - The prompt to use in the chat messages
    # + td - Type descriptor specifying the expected return type format
    # + return - Generates a value that belongs to the type, or an error if generation fails
    isolated remote function generate(ai:Prompt prompt, typedesc<anydata> td = <>) returns td|ai:Error = @java:Method {
        'class: "io.ballerina.lib.ai.azure.Generator"
    } external;

    private isolated function mapToChatCompletionRequestMessage(ai:ChatMessage[]|ai:ChatUserMessage messages)
        returns chat:ChatCompletionRequestMessage[]|ai:Error {
        chat:ChatCompletionRequestMessage[] chatCompletionRequestMessages = [];
        if messages is ai:ChatUserMessage {
            chatCompletionRequestMessages.push(check self.mapToAzureChatMessage(messages));
            return chatCompletionRequestMessages;
        }
        foreach ai:ChatMessage message in messages {
            if message is ai:ChatUserMessage|ai:ChatSystemMessage {
                chatCompletionRequestMessages.push(check self.mapToAzureChatMessage(message));
            } else if message is ai:ChatAssistantMessage {
                chat:ChatCompletionRequestMessage assistantMessage = {role: ai:ASSISTANT};
                ai:FunctionCall[]? toolCalls = message.toolCalls;
                if toolCalls is ai:FunctionCall[] {
                    assistantMessage["function_call"] = {
                        name: toolCalls[0].name,
                        arguments: toolCalls[0].arguments.toJsonString()
                    };
                }
                if message?.content is string {
                    assistantMessage["content"] = message?.content;
                }
                chatCompletionRequestMessages.push(assistantMessage);
            } else if message is ai:ChatFunctionMessage {
                chatCompletionRequestMessages.push(message);
            }
        }
        return chatCompletionRequestMessages;
    }

    private isolated function mapToFunctionCall(chat:ChatCompletionFunctionCall functionCall)
    returns ai:FunctionCall|ai:LlmError {
        do {
            json jsonArgs = check functionCall.arguments.fromJsonString();
            map<json>? arguments = check jsonArgs.cloneWithType();
            return {name: functionCall.name, arguments};
        } on fail error e {
            return error ai:LlmError("Invalid or malformed arguments received in function call response.", e);
        }
    }

    private isolated function mapToAzureChatMessage(ai:ChatUserMessage|ai:ChatSystemMessage message)
    returns AzureChatUserMessage|AzureChatSystemMessage|ai:Error {
        if message is ai:ChatSystemMessage {
            return {
                role: ai:SYSTEM,
                content: check getChatMessageStringContent(message.content),
                name: message.name
            };
        }
        return {
            role: ai:USER,
            content: check getChatMessageStringContent(message.content),
            name: message.name
        };
    }
}

isolated function getChatMessageStringContent(ai:Prompt|string prompt) returns string|ai:Error {
    if prompt is string {
        return prompt;
    }
    string[] & readonly strings = prompt.strings;
    anydata[] insertions = prompt.insertions;
    string promptStr = strings[0];
    foreach int i in 0 ..< insertions.length() {
        string str = strings[i + 1];
        anydata insertion = insertions[i];

        if insertion is ai:TextDocument|ai:TextChunk {
            promptStr += insertion.content + " " + str;
            continue;
        }

        if insertion is ai:TextDocument[] {
            foreach ai:TextDocument doc in insertion {
                promptStr += doc.content + " ";
            }
            promptStr += str;
            continue;
        }

        if insertion is ai:TextChunk[] {
            foreach ai:TextChunk doc in insertion {
                promptStr += doc.content + " ";
            }
            promptStr += str;
            continue;
        }

        if insertion is ai:Document {
            return error ai:Error("Only Text Documents are currently supported.");
        }

        promptStr += insertion.toString() + str;
    }
    return promptStr.trim();
}
